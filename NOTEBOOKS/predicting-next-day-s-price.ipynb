{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9705963,"sourceType":"datasetVersion","datasetId":5936111}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-19T09:10:35.069635Z","iopub.execute_input":"2024-11-19T09:10:35.070027Z","iopub.status.idle":"2024-11-19T09:10:36.375345Z","shell.execute_reply.started":"2024-11-19T09:10:35.069992Z","shell.execute_reply":"2024-11-19T09:10:36.374261Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.impute import SimpleImputer\n\n# Load your dataset\ndata = pd.read_csv('/kaggle/input/financial-data/financial_regression.csv')\n\n# Check if 'high-low' exists, otherwise create it\nif 'high-low' not in data.columns:\n    data['high-low'] = data['sp500 high'] - data['sp500 low']  # Adjust as per the columns you have\n\n# Create the 'next_day_close' column by shifting the 'close' column by 1\ndata['next_day_close'] = data['sp500 close'].shift(-1)\n\n# Drop the last row because the next day's close for it will be NaN\ndata = data.dropna(subset=['next_day_close'])\n\n# Define features and target column\nfeatures = ['sp500 open', 'sp500 high', 'sp500 low', 'sp500 volume', 'high-low']  # Example features\ntarget = 'next_day_close'  # Target variable\n\n# Handling missing values by imputing with mean\nimputer = SimpleImputer(strategy='mean')\n\n# Apply imputation to the features (X) and target (y)\nX = data[features]\ny = data[target]\n\n# Impute missing values in features (X) and target (y)\nX_imputed = imputer.fit_transform(X)  # Impute missing values in X (features)\ny_imputed = imputer.fit_transform(y.values.reshape(-1, 1))  # Impute missing values in y (target)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_imputed, y_imputed, test_size=0.2, random_state=42)\n\n# Initialize and train a Linear Regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Print evaluation results\nprint(f\"Mean Squared Error (MSE): {mse}\")\nprint(f\"R-squared (R2): {r2}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:15:57.360265Z","iopub.execute_input":"2024-11-19T09:15:57.361626Z","iopub.status.idle":"2024-11-19T09:15:57.450807Z","shell.execute_reply.started":"2024-11-19T09:15:57.361579Z","shell.execute_reply":"2024-11-19T09:15:57.449521Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.impute import SimpleImputer\n\n\n\n# Check if 'high-low' exists, otherwise create it\nif 'high-low' not in data.columns:\n    data['high-low'] = data['sp500 high'] - data['sp500 low']  # Adjust as per the columns you have\n\n# Create the 'next_day_close' column by shifting the 'close' column by 1\ndata['next_day_close'] = data['sp500 close'].shift(-1)\n\n# Drop the last row because the next day's close for it will be NaN\ndata = data.dropna(subset=['next_day_close'])\n\n# Define features and target column\nfeatures = ['sp500 open', 'sp500 high', 'sp500 low', 'sp500 volume', 'high-low']  # Example features\ntarget = 'next_day_close'  # Target variable\n\n# Handling missing values by imputing with mean\nimputer = SimpleImputer(strategy='mean')\n\n# Apply imputation to the features (X) and target (y)\nX = data[features]\ny = data[target]\n\n# Impute missing values in features (X) and target (y)\nX_imputed = imputer.fit_transform(X)  # Impute missing values in X (features)\ny_imputed = imputer.fit_transform(y.values.reshape(-1, 1))  # Impute missing values in y (target)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_imputed, y_imputed, test_size=0.2, random_state=42)\n\n# Initialize a model (Ridge or Lasso)\nmodel = Ridge()  # You can replace Ridge with Lasso or LinearRegression\n\n# Set up the parameter grid for tuning\nparam_grid = {\n    'alpha': [0.1, 1, 10, 100],  # Regularization strength for Ridge and Lasso\n}\n\n# Initialize GridSearchCV\ngrid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n\n# Perform the grid search\ngrid_search.fit(X_train, y_train)\n\n# Print the best parameters and best score\nprint(\"Best Hyperparameters:\", grid_search.best_params_)\nprint(\"Best Cross-Validation Score (MSE):\", -grid_search.best_score_)\n\n# Get the best model from the grid search\nbest_model = grid_search.best_estimator_\n\n# Make predictions\ny_pred = best_model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Print evaluation results\nprint(f\"Mean Squared Error (MSE): {mse}\")\nprint(f\"R-squared (R2): {r2}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:17:39.745394Z","iopub.execute_input":"2024-11-19T09:17:39.745829Z","iopub.status.idle":"2024-11-19T09:17:39.869792Z","shell.execute_reply.started":"2024-11-19T09:17:39.745792Z","shell.execute_reply":"2024-11-19T09:17:39.864838Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.impute import SimpleImputer\n\n# Load your dataset\ndata = pd.read_csv('/kaggle/input/financial-data/financial_regression.csv')\n\n# Check if 'high-low' exists, otherwise create it\nif 'high-low' not in data.columns:\n    data['high-low'] = data['sp500 high'] - data['sp500 low']  # Adjust as per the columns you have\n\n# Create the 'next_day_close' column by shifting the 'close' column by 1\ndata['next_day_close'] = data['sp500 close'].shift(-1)\n\n# Drop the last row because the next day's close for it will be NaN\ndata = data.dropna(subset=['next_day_close'])\n\n# Define features and target column\nfeatures = ['sp500 open', 'sp500 high', 'sp500 low', 'sp500 volume', 'high-low']  # Example features\ntarget = 'next_day_close'  # Target variable\n\n# Handling missing values by imputing with mean\nimputer = SimpleImputer(strategy='mean')\n\n# Apply imputation to the features (X) and target (y)\nX = data[features]\ny = data[target]\n\n# Impute missing values in features (X) and target (y)\nX_imputed = imputer.fit_transform(X)  # Impute missing values in X (features)\ny_imputed = imputer.fit_transform(y.values.reshape(-1, 1))  # Impute missing values in y (target)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_imputed, y_imputed, test_size=0.2, random_state=42)\n\n# Initialize models\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42)\ngb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\nxgb_model = XGBRegressor(n_estimators=100, random_state=42)\n\n# Train the models\nrf_model.fit(X_train, y_train)\ngb_model.fit(X_train, y_train)\nxgb_model.fit(X_train, y_train)\n\n# Make predictions\nrf_pred = rf_model.predict(X_test)\ngb_pred = gb_model.predict(X_test)\nxgb_pred = xgb_model.predict(X_test)\n\n# Evaluate the models\nrf_mse = mean_squared_error(y_test, rf_pred)\ngb_mse = mean_squared_error(y_test, gb_pred)\nxgb_mse = mean_squared_error(y_test, xgb_pred)\n\nrf_r2 = r2_score(y_test, rf_pred)\ngb_r2 = r2_score(y_test, gb_pred)\nxgb_r2 = r2_score(y_test, xgb_pred)\n\n# Print results\nprint(f\"Random Forest MSE: {rf_mse}, R2: {rf_r2}\")\nprint(f\"Gradient Boosting MSE: {gb_mse}, R2: {gb_r2}\")\nprint(f\"XGBoost MSE: {xgb_mse}, R2: {xgb_r2}\")\n\n# Now let's apply ensemble predictions by averaging\nensemble_pred = (rf_pred + gb_pred + xgb_pred) / 3\n\n# Evaluate ensemble model\nensemble_mse = mean_squared_error(y_test, ensemble_pred)\nensemble_r2 = r2_score(y_test, ensemble_pred)\n\nprint(f\"Ensemble Model MSE: {ensemble_mse}, R2: {ensemble_r2}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-19T09:19:03.813038Z","iopub.execute_input":"2024-11-19T09:19:03.813452Z","iopub.status.idle":"2024-11-19T09:19:06.872167Z","shell.execute_reply.started":"2024-11-19T09:19:03.813412Z","shell.execute_reply":"2024-11-19T09:19:06.871343Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}